{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kerialexander/Project-3/blob/main/Tutoring_2_3_25_Version_45.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary imports\n",
        "import tensorflow as tf\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RuLFek2a17L7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b3d19b-7ac0-43cc-9274-292ecc92a21c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import other libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "import torch"
      ],
      "metadata": {
        "id": "thBvRQkoNA3A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y7au-KxdPGAA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "2ee6d623-e19f-4e00-e4fe-1670a949f39d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "### **2. Preprocess Data**\n",
        "\n",
        "# Dummy functions for pose estimation and segmentation\n",
        "def estimate_pose(image):\n",
        "    # Dummy implementation for pose estimation\n",
        "    return \"pose_map\"\n",
        "\n",
        "def segment_person(image):\n",
        "    # Dummy implementation for person segmentation\n",
        "    return \"segmentation_map\"\n",
        "\n",
        "# Function to preprocess person images\n",
        "def preprocess_person(image_path):\n",
        "    person_image = cv2.imread(image_path)\n",
        "    if person_image is None:\n",
        "        raise FileNotFoundError(f\"Person image not found at path: {image_path}\")\n",
        "    pose_map = estimate_pose(person_image)  # Pose estimation function\n",
        "    segmentation_map = segment_person(person_image)  # Segmentation function\n",
        "    return pose_map, segmentation_map\n",
        "\n",
        "# Function to preprocess clothing images\n",
        "def preprocess_clothing(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((1024, 768)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    clothing_image = cv2.imread(image_path)\n",
        "    if clothing_image is None:\n",
        "        raise FileNotFoundError(f\"Clothing image not found at path: {image_path}\")\n",
        "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
        "    clothing_image = transform(clothing_image)\n",
        "    return clothing_image\n",
        "\n",
        "# Define the paths to the person and clothing images\n",
        "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Show image from clothing_image_path\n",
        "clothing_image = cv2.imread(clothing_image_path)\n",
        "plt.imshow(clothing_image)\n",
        "\n",
        "# Preprocess the images\n",
        "pose_map, segmentation_map = preprocess_person(person_image_path)\n",
        "clothing_image = preprocess_clothing(clothing_image_path)\n",
        "\n",
        "try:\n",
        "    pose_map, segmentation_map = preprocess_person(person_image_path)\n",
        "except FileNotFoundError:\n",
        "    print(\"Person image not found. Using dummy pose and segmentation maps.\")\n",
        "    pose_map, segmentation_map = \"pose_map\", \"segmentation_map\"\n",
        "\n",
        "try:\n",
        "    clothing_image = preprocess_clothing(clothing_image_path)\n",
        "except FileNotFoundError:\n",
        "    print(\"Clothing image not found. Using a dummy tensor.\")\n",
        "    clothing_image = torch.zeros((3, 1024, 768))  # Dummy tensor with the same shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VDwJJA2PF__",
        "outputId": "f8ef9919-3b83-4b1a-f014-829ca13467ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "## VITON-HD Virtual Try-On Model - Jupyter Notebook\n",
        "\n",
        "### **1. Setup the Environment**\n",
        "\n",
        "# Install dependencies\n",
        "%pip install torch torchvision numpy matplotlib opencv-python\n",
        "\n",
        "# Note: Creating and activating a virtual environment should be done in a terminal, not in a Jupyter Notebook cell.\n",
        "# The following commands are for reference and should be run in a terminal:\n",
        "# !conda create -n viton_hd python=3.7 -y\n",
        "# !conda activate viton_hd\n",
        "\n",
        "import torch\n",
        "\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.sum() > 0:\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "model = MyModel()\n",
        "scripted_model = torch.jit.script(model)\n",
        "\n",
        "x = torch.randn(1, 10)\n",
        "output = scripted_model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Mask R-CNN repository.\n",
        "\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5"
      ],
      "metadata": {
        "id": "ciJ14i6U_9re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216b670e-9bcf-46c0-8b3f-d613d1f2a0ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-04 23:52:42--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250204T235243Z&X-Amz-Expires=300&X-Amz-Signature=59c2fc6baa605a2b0a0c8282fe7dc9346e8d18849855d53f1ad9115a680951db&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-04 23:52:43--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250204T235243Z&X-Amz-Expires=300&X-Amz-Signature=59c2fc6baa605a2b0a0c8282fe7dc9346e8d18849855d53f1ad9115a680951db&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257557808 (246M) [application/octet-stream]\n",
            "Saving to: mask_rcnn_coco.h5.1\n",
            "\n",
            "mask_rcnn_coco.h5.1 100%[===================>] 245.63M   171MB/s    in 1.4s    \n",
            "\n",
            "2025-02-04 23:52:44 (171 MB/s) - mask_rcnn_coco.h5.1 saved [257557808/257557808]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) # Remount with force_remount=True"
      ],
      "metadata": {
        "id": "l553ndwLYOsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e47d889-31f7-4a7b-d5d5-fb08544bd642"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision numpy matplotlib opencv-python\n",
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "!pip install scikit-image\n",
        "!pip install mediapipe opencv-python\n",
        "!pip install mediapipe --upgrade\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the Mask R-CNN repository\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "from skimage.transform import PiecewiseAffineTransform, warp\n",
        "import mediapipe as mp\n",
        "\n",
        "# Define paths to images\n",
        "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Function to preprocess person images\n",
        "def preprocess_person(image_path):\n",
        "    person_image = cv2.imread(image_path)\n",
        "    if person_image is None:\n",
        "        raise FileNotFoundError(f\"Person image not found at path: {image_path}\")\n",
        "    return person_image\n",
        "\n",
        "# Function to preprocess clothing images\n",
        "def preprocess_clothing(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((1024, 768)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    clothing_image = cv2.imread(image_path)\n",
        "    if clothing_image is None:\n",
        "        raise FileNotFoundError(f\"Clothing image not found at path: {image_path}\")\n",
        "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
        "    clothing_image = transform(clothing_image)\n",
        "    return clothing_image\n",
        "\n",
        "# Function to segment clothing\n",
        "def segment_clothing(image_path):\n",
        "    \"\"\"Segments the clothing (gown) in the image using PixelLib.\"\"\"\n",
        "    segment_image = instance_segmentation()\n",
        "    model_path = \"/content/mask_rcnn_coco.h5\"  # Changed to likely download location\n",
        "    segment_image.load_model(model_path)\n",
        "    target_classes = segment_image.select_target_classes(dress=True)\n",
        "    results = segment_image.segmentImage(image_path, segment_target_classes=target_classes,\n",
        "                                        output_image_name=\"segmented_clothing.jpg\", show_bboxes=True)\n",
        "\n",
        "    mask = results[1]['masks']\n",
        "    if mask.shape[2] > 0:\n",
        "        clothing_mask = mask[:, :, 0]\n",
        "    else:\n",
        "        clothing_mask = np.zeros_like(results[0], dtype=np.uint8)\n",
        "\n",
        "    return clothing_mask\n",
        "\n",
        "# Function to create transparent clothing image\n",
        "def create_transparent_clothing_image(image_path, clothing_mask):\n",
        "    \"\"\"Creates a transparent clothing image using the mask.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    transparent_image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
        "    transparent_image[clothing_mask == 0, 3] = 0\n",
        "    return transparent_image\n",
        "\n",
        "# Function to approximate gown segmentation\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    lower_color = np.array([0, 0, 150])\n",
        "    upper_color = np.array([180, 50, 255])\n",
        "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# Preprocess the images\n",
        "person_image = preprocess_person(person_image_path)\n",
        "clothing_image = preprocess_clothing(clothing_image_path)\n",
        "\n",
        "# Segment the clothing\n",
        "clothing_mask = segment_clothing(clothing_image_path)\n",
        "\n",
        "# Create transparent clothing image\n",
        "transparent_clothing_image = create_transparent_clothing_image(clothing_image_path, clothing_mask)\n",
        "\n",
        "# Display and save the transparent image\n",
        "plt.imshow(cv2.cvtColor(transparent_clothing_image, cv2.COLOR_BGRA2RGBA))\n",
        "plt.title(\"Transparent Clothing Image\")\n",
        "plt.show()\n",
        "cv2.imwrite(\"transparent_clothing.png\", transparent_clothing_image)\n",
        "\n",
        "# Show original images\n",
        "clothing_image_original = cv2.imread(clothing_image_path)\n",
        "plt.imshow(clothing_image_original)\n",
        "plt.title(\"Original Clothing Image\")\n",
        "plt.show()\n",
        "\n",
        "person_image_original = cv2.imread(person_image_path)\n",
        "plt.imshow(person_image_original)\n",
        "plt.title(\"Original Person Image\")\n",
        "plt.show()\n",
        "\n",
        "# --- Warping and Blending ---\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image_original)  # Use original clothing image\n",
        "gown_image = cv2.bitwise_and(clothing_image_original, clothing_image_original, mask=gown_mask)\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "        dst_pts = np.float32([\n",
        "            [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "            [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "            [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "            [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]],\n",
        "        ])\n",
        "\n",
        "        # Calculate the perspective transformation matrix\n",
        "        M = cv2.getPerspectiveTransform(gown_coords, dst_pts)\n",
        "\n",
        "        # Warp the gown image\n",
        "        warped_gown = cv2.warpPerspective(clothing_image_original, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "        # 4. Blending with background correction\n",
        "        warped_gown_mask = cv2.cvtColor(warped_gown, cv2.COLOR_BGR2GRAY)\n",
        "        warped_gown_mask = warped_gown_mask > 0\n",
        "        person_image_with_hole = person_image.copy()\n",
        "        person_image_with_hole[warped_gown_mask] = warped_gown[warped_gown_mask]\n",
        "\n",
        "        # 5. Display Results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(person_image_with_hole)\n",
        "        plt.title(\"Warped and Blended Image\")\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Pose landmarks not detected in the person image.\")\n",
        "else:\n",
        "    print(\"No contours detected in the clothing image. Check segmentation.\")"
      ],
      "metadata": {
        "id": "N4B1nTNJKs2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb241771-5a51-40fa-f6d7-a91e5a61bf96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pixellib in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pixellib) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.25.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from pixellib) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.10.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.4.0)\n",
            "Requirement already satisfied: labelme2coco in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.2.6)\n",
            "Requirement already satisfied: imantics in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.12)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.0.11)\n",
            "Requirement already satisfied: pyQt5 in /usr/local/lib/python3.11/dist-packages (from pixellib) (5.15.11)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.10)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from pixellib) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pixellib) (2.5.0)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pixellib) (4.67.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pixellib) (1.0.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.0.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.6)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.11/dist-packages (from pixellib) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.11/dist-packages (from pixellib) (25.1.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (8.1.8)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (24.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (4.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pixellib) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pixellib) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pixellib) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->pixellib) (3.1.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core->pixellib) (4.9.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from imantics->pixellib) (5.3.0)\n",
            "Requirement already satisfied: xmljson in /usr/local/lib/python3.11/dist-packages (from imantics->pixellib) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (1.13.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (2.36.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (2.0.6)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pixellib) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pixellib) (2025.1.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pixellib) (0.4)\n",
            "Requirement already satisfied: sahi>=0.8.19 in /usr/local/lib/python3.11/dist-packages (from labelme2coco->pixellib) (0.11.20)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from labelme2coco->pixellib) (4.23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (2.8.2)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.15 in /usr/local/lib/python3.11/dist-packages (from pyQt5->pixellib) (12.17.0)\n",
            "Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /usr/local/lib/python3.11/dist-packages (from pyQt5->pixellib) (5.15.16)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (0.22.3)\n",
            "Requirement already satisfied: pybboxes==0.1.6 in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (0.1.6)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (0.7.0)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (3.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (2024.12.14)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.1.10)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.20)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.20)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--2025-02-04 23:53:33--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250204T235333Z&X-Amz-Expires=300&X-Amz-Signature=7674aa84da6c3c7ab5a06ee32f3be17ac0ae62f5cea47622239590524e5c70a0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-04 23:53:33--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250204T235333Z&X-Amz-Expires=300&X-Amz-Signature=7674aa84da6c3c7ab5a06ee32f3be17ac0ae62f5cea47622239590524e5c70a0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257557808 (246M) [application/octet-stream]\n",
            "Saving to: mask_rcnn_coco.h5.2\n",
            "\n",
            "mask_rcnn_coco.h5.2 100%[===================>] 245.63M   210MB/s    in 1.2s    \n",
            "\n",
            "2025-02-04 23:53:35 (210 MB/s) - mask_rcnn_coco.h5.2 saved [257557808/257557808]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "numpy() is only available when eager execution is enabled.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eea9b7286db4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Segment the clothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mclothing_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_clothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclothing_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Create transparent clothing image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-eea9b7286db4>\u001b[0m in \u001b[0;36msegment_clothing\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0msegment_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/mask_rcnn_coco.h5\"\u001b[0m  \u001b[0;31m# Changed to likely download location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msegment_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mtarget_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_target_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     results = segment_image.segmentImage(image_path, segment_target_classes=target_classes,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pixellib/instance/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path, confidence)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inference\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pixellib/instance/mask_rcnn.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m                 \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2111\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m                 \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group_by_name\u001b[0;34m(f, layers, skip_mismatch)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0msymbolic_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m       weight_values = preprocess_weights_for_loading(\n\u001b[1;32m    768\u001b[0m           layer, weight_values, original_keras_version, original_backend)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36m_legacy_weights\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;34m'Save or restore weights that is not an instance of `tf.Variable` is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;34m'not supported in h5, use `save_format=\\'tf\\'` instead. Got a model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         'or layer {} with weights {}'.format(layer.__class__.__name__, weights))\n\u001b[0m\u001b[1;32m    899\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_value\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mvalue_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\", value={value}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         return (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36mconvert_to_numpy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;31m# the same error. The `EagerTensor` class must be doing something behind the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;31m# scenes to make `np.array(tf.constant(1))` work.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_asarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision numpy matplotlib opencv-python\n",
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "!pip install scikit-image\n",
        "!pip install mediapipe opencv-python\n",
        "!pip install mediapipe --upgrade\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the Mask R-CNN repository\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "from skimage.transform import PiecewiseAffineTransform, warp\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable eager execution before any other TensorFlow operations\n",
        "# This should be done at the very beginning of your notebook or script\n",
        "tf.compat.v1.enable_eager_execution() # Moved to the beginning\n",
        "\n",
        "# Define paths to images\n",
        "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Function to preprocess person images\n",
        "def preprocess_person(image_path):\n",
        "    person_image = cv2.imread(image_path)\n",
        "    if person_image is None:\n",
        "        raise FileNotFoundError(f\"Person image not found at path: {image_path}\")\n",
        "    return person_image\n",
        "\n",
        "# Function to preprocess clothing images\n",
        "def preprocess_clothing(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((1024, 768)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    clothing_image = cv2.imread(image_path)\n",
        "    if clothing_image is None:\n",
        "        raise FileNotFoundError(f\"Clothing image not found at path: {image_path}\")\n",
        "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
        "    clothing_image = transform(clothing_image)\n",
        "    return clothing_image\n",
        "\n",
        "# Function to segment clothing\n",
        "def segment_clothing(image_path):\n",
        "    \"\"\"Segments the clothing (gown) in the image using PixelLib.\"\"\"\n",
        "    segment_image = instance_segmentation()\n",
        "    model_path = \"/content/mask_rcnn_coco.h5\"  # Changed to likely download location\n",
        "    segment_image.load_model(model_path)\n",
        "    target_classes = segment_image.select_target_classes(dress=True)\n",
        "    results = segment_image.segmentImage(image_path, segment_target_classes=target_classes,\n",
        "                                        output_image_name=\"segmented_clothing.jpg\", show_bboxes=True)\n",
        "\n",
        "    mask = results[1]['masks']\n",
        "    if mask.shape[2] > 0:\n",
        "        clothing_mask = mask[:, :, 0]\n",
        "    else:\n",
        "        clothing_mask = np.zeros_like(results[0], dtype=np.uint8)\n",
        "\n",
        "    return clothing_mask\n",
        "\n",
        "# Function to create transparent clothing image\n",
        "def create_transparent_clothing_image(image_path, clothing_mask):\n",
        "    \"\"\"Creates a transparent clothing image using the mask.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    transparent_image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
        "    transparent_image[clothing_mask == 0, 3] = 0\n",
        "    return transparent_image\n",
        "\n",
        "# Function to approximate gown segmentation\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    lower_color = np.array([0, 0, 150])\n",
        "    upper_color = np.array([180, 50, 255])\n",
        "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# Preprocess the images\n",
        "person_image = preprocess_person(person_image_path)\n",
        "clothing_image = preprocess_clothing(clothing_image_path)\n",
        "\n",
        "# Segment the clothing\n",
        "clothing_mask = segment_clothing(clothing_image_path)\n",
        "\n",
        "# Create transparent clothing image\n",
        "transparent_clothing_image = create_transparent_clothing_image(clothing_image_path, clothing_mask)\n",
        "\n",
        "# Display and save the transparent image\n",
        "plt.imshow(cv2.cvtColor(transparent_clothing_image, cv2.COLOR_BGRA2RGBA))\n",
        "plt.title(\"Transparent Clothing Image\")\n",
        "plt.show()\n",
        "cv2.imwrite(\"transparent_clothing.png\", transparent_clothing_image)\n",
        "\n",
        "# Show original images\n",
        "clothing_image_original = cv2.imread(clothing_image_path)\n",
        "plt.imshow(clothing_image_original)\n",
        "plt.title(\"Original Clothing Image\")\n",
        "plt.show()\n",
        "\n",
        "person_image_original = cv2.imread(person_image_path)\n",
        "plt.imshow(person_image_original)\n",
        "plt.title(\"Original Person Image\")\n",
        "plt.show()\n",
        "\n",
        "# --- Warping and Blending ---\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image_original)  # Use original clothing image\n",
        "gown_image = cv2.bitwise_and(clothing_image_original, clothing_image_original, mask=gown_mask)\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "        dst_pts = np.float32([\n",
        "            [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "            [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "            [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "            [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]],\n",
        "        ])\n",
        "\n",
        "        # Calculate the perspective transformation matrix\n",
        "        M = cv2.getPerspectiveTransform(gown_coords, dst_pts)\n",
        "\n",
        "        # Warp the gown image\n",
        "        warped_gown = cv2.warpPerspective(clothing_image_original, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "        # 4. Blending with background correction\n",
        "        warped_gown_mask = cv2.cvtColor(warped_gown, cv2.COLOR_BGR2GRAY)\n",
        "        warped_gown_mask = warped_gown_mask > 0\n",
        "        person_image_with_hole = person_image.copy()\n",
        "        person_image_with_hole[warped_gown_mask] = warped_gown[warped_gown_mask]\n",
        "\n",
        "        # 5. Display Results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(person_image_with_hole)\n",
        "        plt.title(\"Warped and Blended Image\")\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Pose landmarks not detected in the person image.\")\n",
        "else:\n",
        "    print(\"No contours detected in the clothing image. Check segmentation.\")"
      ],
      "metadata": {
        "id": "E_yaufmoIFct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision numpy matplotlib opencv-python\n",
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "!pip install scikit-image\n",
        "!pip install mediapipe opencv-python\n",
        "!pip install mediapipe --upgrade\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the Mask R-CNN repository\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "from skimage.transform import PiecewiseAffineTransform, warp\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable eager execution before any other TensorFlow operations\n",
        "# This should be done at the very beginning of your notebook or script\n",
        "tf.compat.v1.enable_eager_execution() # Moved to the beginning\n",
        "\n",
        "# Define paths to images\n",
        "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Function to preprocess person images\n",
        "def preprocess_person(image_path):\n",
        "    person_image = cv2.imread(image_path)\n",
        "    if person_image is None:\n",
        "        raise FileNotFoundError(f\"Person image not found at path: {image_path}\")\n",
        "    return person_image\n",
        "\n",
        "# Function to preprocess clothing images\n",
        "def preprocess_clothing(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((1024, 768)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    clothing_image = cv2.imread(image_path)\n",
        "    if clothing_image is None:\n",
        "        raise FileNotFoundError(f\"Clothing image not found at path: {image_path}\")\n",
        "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
        "    clothing_image = transform(clothing_image)\n",
        "    return clothing_image\n",
        "\n",
        "# Function to segment clothing\n",
        "def segment_clothing(image_path):\n",
        "    \"\"\"Segments the clothing (gown) in the image using PixelLib.\"\"\"\n",
        "    segment_image = instance_segmentation()\n",
        "    model_path = \"/content/mask_rcnn_coco.h5\"  # Changed to likely download location\n",
        "    segment_image.load_model(model_path)\n",
        "    target_classes = segment_image.select_target_classes(dress=True)\n",
        "    results = segment_image.segmentImage(image_path, segment_target_classes=target_classes,\n",
        "                                        output_image_name=\"segmented_clothing.jpg\", show_bboxes=True)\n",
        "\n",
        "    mask = results[1]['masks']\n",
        "    if mask.shape[2] > 0:\n",
        "        clothing_mask = mask[:, :, 0]\n",
        "    else:\n",
        "        clothing_mask = np.zeros_like(results[0], dtype=np.uint8)\n",
        "\n",
        "    return clothing_mask\n",
        "\n",
        "# Function to create transparent clothing image\n",
        "def create_transparent_clothing_image(image_path, clothing_mask):\n",
        "    \"\"\"Creates a transparent clothing image using the mask.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    transparent_image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
        "    transparent_image[clothing_mask == 0, 3] = 0\n",
        "    return transparent_image\n",
        "\n",
        "# Function to approximate gown segmentation\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    lower_color = np.array([0, 0, 150])\n",
        "    upper_color = np.array([180, 50, 255])\n",
        "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# Preprocess the images\n",
        "person_image = preprocess_person(person_image_path)\n",
        "clothing_image = preprocess_clothing(clothing_image_path)\n",
        "\n",
        "# Segment the clothing\n",
        "clothing_mask = segment_clothing(clothing_image_path)\n",
        "\n",
        "# Create transparent clothing image\n",
        "transparent_clothing_image = create_transparent_clothing_image(clothing_image_path, clothing_mask)\n",
        "\n",
        "# Display and save the transparent image\n",
        "plt.imshow(cv2.cvtColor(transparent_clothing_image, cv2.COLOR_BGRA2RGBA))\n",
        "plt.title(\"Transparent Clothing Image\")\n",
        "plt.show()\n",
        "cv2.imwrite(\"transparent_clothing.png\", transparent_clothing_image)\n",
        "\n",
        "# Show original images\n",
        "clothing_image_original = cv2.imread(clothing_image_path)\n",
        "plt.imshow(clothing_image_original)\n",
        "plt.title(\"Original Clothing Image\")\n",
        "plt.show()\n",
        "\n",
        "person_image_original = cv2.imread(person_image_path)\n",
        "plt.imshow(person_image_original)\n",
        "plt.title(\"Original Person Image\")\n",
        "plt.show()\n",
        "\n",
        "# --- Warping and Blending ---\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image_original)  # Use original clothing image\n",
        "gown_image = cv2.bitwise_and(clothing_image_original, clothing_image_original, mask=gown_mask)\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "        dst_pts = np.float32([\n",
        "            [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "            [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "            [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "            [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]],\n",
        "        ])\n",
        "\n",
        "        # Calculate the perspective transformation matrix\n",
        "        M = cv2.getPerspectiveTransform(gown_coords, dst_pts)\n",
        "\n",
        "        # Warp the gown image\n",
        "        warped_gown = cv2.warpPerspective(clothing_image_original, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "        # 4. Blending with background correction\n",
        "        warped_gown_mask = cv2.cvtColor(warped_gown, cv2.COLOR_BGR2GRAY)\n",
        "        warped_gown_mask = warped_gown_mask > 0\n",
        "        person_image_with_hole = person_image.copy()\n",
        "        person_image_with_hole[warped_gown_mask] = warped_gown[warped_gown_mask]\n",
        "\n",
        "        # 5. Display Results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(person_image_with_hole)\n",
        "        plt.title(\"Warped and Blended Image\")\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Pose landmarks not detected in the person image.\")\n",
        "else:\n",
        "    print(\"No contours detected in the clothing image. Check segmentation.\")"
      ],
      "metadata": {
        "id": "co961tOwH03_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision numpy matplotlib opencv-python\n",
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "!pip install scikit-image\n",
        "!pip install mediapipe opencv-python\n",
        "!pip install mediapipe --upgrade\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the Mask R-CNN repository\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "from skimage.transform import PiecewiseAffineTransform, warp\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable eager execution before any other TensorFlow operations\n",
        "# This should be done at the very beginning of your notebook or script\n",
        "tf.compat.v1.enable_eager_execution() # Moved to the beginning\n",
        "\n",
        "# Define paths to images\n",
        "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Install dependencies\n",
        "!pip install torch torchvision numpy matplotlib opencv-python\n",
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "!pip install scikit-image\n",
        "!pip install mediapipe opencv-python\n",
        "!pip install mediapipe --upgrade\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the Mask R-CNN repository\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "from skimage.transform import PiecewiseAffineTransform, warp\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths to images\n",
        "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Function to preprocess person images\n",
        "def preprocess_person(image_path):\n",
        "    person_image = cv2.imread(image_path)\n",
        "    if person_image is None:\n",
        "        raise FileNotFoundError(f\"Person image not found at path: {image_path}\")\n",
        "    return person_image\n",
        "\n",
        "# Function to preprocess clothing images\n",
        "def preprocess_clothing(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((1024, 768)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    clothing_image = cv2.imread(image_path)\n",
        "    if clothing_image is None:\n",
        "        raise FileNotFoundError(f\"Clothing image not found at path: {image_path}\")\n",
        "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
        "    clothing_image = transform(clothing_image)\n",
        "    return clothing_image\n",
        "\n",
        "# Function to segment clothing\n",
        "def segment_clothing(image_path):\n",
        "    \"\"\"Segments the clothing (gown) in the image using PixelLib.\"\"\"\n",
        "    segment_image = instance_segmentation()\n",
        "    model_path = \"/content/mask_rcnn_coco.h5\"  # Changed to likely download location\n",
        "    segment_image.load_model(model_path)\n",
        "    target_classes = segment_image.select_target_classes(dress=True)\n",
        "    results = segment_image.segmentImage(image_path, segment_target_classes=target_classes,\n",
        "                                        output_image_name=\"segmented_clothing.jpg\", show_bboxes=True)\n",
        "\n",
        "    mask = results[1]['masks']\n",
        "    if mask.shape[2] > 0:\n",
        "        clothing_mask = mask[:, :, 0]\n",
        "    else:\n",
        "        clothing_mask = np.zeros_like(results[0], dtype=np.uint8)\n",
        "\n",
        "    return clothing_mask\n",
        "\n",
        "# Function to create transparent clothing image\n",
        "def create_transparent_clothing_image(image_path, clothing_mask):\n",
        "    \"\"\"Creates a transparent clothing image using the mask.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    transparent_image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
        "    transparent_image[clothing_mask == 0, 3] = 0\n",
        "    return transparent_image\n",
        "\n",
        "# Function to approximate gown segmentation\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    lower_color = np.array([0, 0, 150])\n",
        "    upper_color = np.array([180, 50, 255])\n",
        "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# Preprocess the images\n",
        "person_image = preprocess_person(person_image_path)\n",
        "clothing_image = preprocess_clothing(clothing_image_path)\n",
        "\n",
        "# Segment the clothing\n",
        "clothing_mask = segment_clothing(clothing_image_path)\n",
        "\n",
        "# Create transparent clothing image\n",
        "transparent_clothing_image = create_transparent_clothing_image(clothing_image_path, clothing_mask)\n",
        "\n",
        "# Display and save the transparent image\n",
        "plt.imshow(cv2.cvtColor(transparent_clothing_image, cv2.COLOR_BGRA2RGBA))\n",
        "plt.title(\"Transparent Clothing Image\")\n",
        "plt.show()\n",
        "cv2.imwrite(\"transparent_clothing.png\", transparent_clothing_image)\n",
        "\n",
        "# Show original images\n",
        "clothing_image_original = cv2.imread(clothing_image_path)\n",
        "plt.imshow(clothing_image_original)\n",
        "plt.title(\"Original Clothing Image\")\n",
        "plt.show()\n",
        "\n",
        "person_image_original = cv2.imread(person_image_path)\n",
        "plt.imshow(person_image_original)\n",
        "plt.title(\"Original Person Image\")\n",
        "plt.show()\n",
        "\n",
        "# --- Warping and Blending ---\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image_original)  # Use original clothing image\n",
        "gown_image = cv2.bitwise_and(clothing_image_original, clothing_image_original, mask=gown_mask)\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "        dst_pts = np.float32([\n",
        "            [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "            [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "            [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "            [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]],\n",
        "        ])\n",
        "\n",
        "        # Calculate the perspective transformation matrix\n",
        "        M = cv2.getPerspectiveTransform(gown_coords, dst_pts)\n",
        "\n",
        "        # Warp the gown image\n",
        "        warped_gown = cv2.warpPerspective(clothing_image_original, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "        # 4. Blending with background correction\n",
        "        warped_gown_mask = cv2.cvtColor(warped_gown, cv2.COLOR_BGR2GRAY)\n",
        "        warped_gown_mask = warped_gown_mask > 0\n",
        "        person_image_with_hole = person_image.copy()\n",
        "        person_image_with_hole[warped_gown_mask] = warped_gown[warped_gown_mask]\n",
        "\n",
        "        # 5. Display Results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(person_image_with_hole)\n",
        "        plt.title(\"Warped and Blended Image\")\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Pose landmarks not detected in the person image.\")\n",
        "else:\n",
        "    print(\"No contours detected in the clothing image. Check segmentation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z85m8m9caDc_",
        "outputId": "623823d0-b553-4a8f-fd7f-7ec8c78a6dff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pixellib in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pixellib) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.25.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from pixellib) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.10.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.4.0)\n",
            "Requirement already satisfied: labelme2coco in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.2.6)\n",
            "Requirement already satisfied: imantics in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.12)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.0.11)\n",
            "Requirement already satisfied: pyQt5 in /usr/local/lib/python3.11/dist-packages (from pixellib) (5.15.11)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.10)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from pixellib) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pixellib) (2.5.0)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pixellib) (4.67.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pixellib) (1.0.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from pixellib) (3.0.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.11/dist-packages (from pixellib) (0.6)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.11/dist-packages (from pixellib) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.11/dist-packages (from pixellib) (25.1.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (8.1.8)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (24.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->pixellib) (4.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pixellib) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pixellib) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pixellib) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->pixellib) (3.1.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core->pixellib) (4.9.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from imantics->pixellib) (5.3.0)\n",
            "Requirement already satisfied: xmljson in /usr/local/lib/python3.11/dist-packages (from imantics->pixellib) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (1.13.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (2.36.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug->pixellib) (2.0.6)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pixellib) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pixellib) (2025.1.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pixellib) (0.4)\n",
            "Requirement already satisfied: sahi>=0.8.19 in /usr/local/lib/python3.11/dist-packages (from labelme2coco->pixellib) (0.11.20)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from labelme2coco->pixellib) (4.23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pixellib) (2.8.2)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.15 in /usr/local/lib/python3.11/dist-packages (from pyQt5->pixellib) (12.17.0)\n",
            "Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /usr/local/lib/python3.11/dist-packages (from pyQt5->pixellib) (5.15.16)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->labelme2coco->pixellib) (0.22.3)\n",
            "Requirement already satisfied: pybboxes==0.1.6 in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (0.1.6)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (0.7.0)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (3.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from sahi>=0.8.19->labelme2coco->pixellib) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->sahi>=0.8.19->labelme2coco->pixellib) (2024.12.14)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.1.10)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.20)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.20)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--2025-02-04 23:56:15--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250204T235615Z&X-Amz-Expires=300&X-Amz-Signature=0a23d9d2022c906c88b1eae9d44977dc19ef63ae25e8e4a339c5071644a79c4d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-04 23:56:15--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250204T235615Z&X-Amz-Expires=300&X-Amz-Signature=0a23d9d2022c906c88b1eae9d44977dc19ef63ae25e8e4a339c5071644a79c4d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257557808 (246M) [application/octet-stream]\n",
            "Saving to: mask_rcnn_coco.h5.3\n",
            "\n",
            "mask_rcnn_coco.h5.3 100%[===================>] 245.63M   165MB/s    in 1.5s    \n",
            "\n",
            "2025-02-04 23:56:17 (165 MB/s) - mask_rcnn_coco.h5.3 saved [257557808/257557808]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "tf.enable_eager_execution must be called at program startup.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-949a4677f12c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Enable eager execution before any other TensorFlow operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# This should be done at the very beginning of your notebook or script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Moved to the beginning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Define paths to images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   4961\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Enabling eager execution\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4962\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_execution_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4963\u001b[0;31m     return enable_eager_execution_internal(\n\u001b[0m\u001b[1;32m   4964\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4965\u001b[0m         \u001b[0mdevice_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution_internal\u001b[0;34m(config, device_policy, execution_mode, server_def)\u001b[0m\n\u001b[1;32m   5025\u001b[0m         _default_graph_stack._global_default_graph is not None)  # pylint: disable=protected-access\n\u001b[1;32m   5026\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph_mode_has_been_used\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5027\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   5028\u001b[0m           \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[1;32m   5029\u001b[0m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_execution_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separate the gown in the image from the background of the image so it is only displaying the clothing with a transparent background.\n",
        "\n",
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def segment_clothing(image_path):\n",
        "    \"\"\"Segments the clothing (gown) in the image using PixelLib.\"\"\"\n",
        "    segment_image = instance_segmentation()\n",
        "    # Update the path to the model file\n",
        "    # Make sure the path below is correct!\n",
        "    # If the file is in your Google Drive, make sure it's in 'My Drive' and the filename is correct.\n",
        "    model_path = \"/content/drive/MyDrive/mask_rcnn_coco.h5\"  # Updated path\n",
        "    segment_image.load_model(model_path)\n",
        "    target_classes = segment_image.select_target_classes(dress=True)\n",
        "    results = segment_image.segmentImage(image_path, segment_target_classes=target_classes,\n",
        "                                        output_image_name=\"segmented_clothing.jpg\", show_bboxes=True)\n",
        "\n",
        "    mask = results[1]['masks']\n",
        "    if mask.shape[2] > 0:\n",
        "        clothing_mask = mask[:, :, 0]\n",
        "    else:\n",
        "        clothing_mask = np.zeros_like(results[0], dtype=np.uint8)\n",
        "\n",
        "    return clothing_mask\n",
        "\n",
        "def create_transparent_clothing_image(image_path, clothing_mask):\n",
        "    \"\"\"Creates a transparent clothing image using the mask.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Create a 4-channel image (BGRA) to store transparency\n",
        "    transparent_image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
        "\n",
        "    # Set alpha channel to 0 where mask is False (background)\n",
        "    transparent_image[clothing_mask == 0, 3] = 0\n",
        "\n",
        "    return transparent_image\n",
        "\n",
        "# Load the clothing image path\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Segment the clothing\n",
        "clothing_mask = segment_clothing(clothing_image_path)\n",
        "\n",
        "# Create transparent clothing image\n",
        "transparent_clothing_image = create_transparent_clothing_image(clothing_image_path, clothing_mask)\n",
        "\n",
        "# Display the transparent image (you might need to save it to see the transparency)\n",
        "plt.imshow(cv2.cvtColor(transparent_clothing_image, cv2.COLOR_BGRA2RGBA))\n",
        "plt.title(\"Transparent Clothing Image\")\n",
        "plt.show()\n",
        "\n",
        "# Save the transparent image (optional)\n",
        "cv2.imwrite(\"transparent_clothing.png\", transparent_clothing_image)"
      ],
      "metadata": {
        "id": "HaRxOlY8BS0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate the gown in the image from the background of the image so it is only displaying the clothing with a transparent background.\n",
        "\n",
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def segment_clothing(image_path):\n",
        "    \"\"\"Segments the clothing (gown) in the image using PixelLib.\"\"\"\n",
        "    segment_image = instance_segmentation()\n",
        "    # Update the path to the model file\n",
        "    model_path = \"/content/drive/My Drive/mask_rcnn_coco.h5\"  # Replace with your path\n",
        "    segment_image.load_model(model_path)\n",
        "\n",
        "# def segment_clothing(image_path):\n",
        "#     \"\"\"Segments the clothing (gown) in the image using PixelLib.\"\"\"\n",
        "#     segment_image = instance_segmentation()\n",
        "#     segment_image.load_model(\"mask_rcnn_coco.h5\")  # Download the model if you haven't already\n",
        "    target_classes = segment_image.select_target_classes(dress=True) # Select the 'dress' class\n",
        "    results = segment_image.segmentImage(image_path, segment_target_classes=target_classes,\n",
        "                                        output_image_name=\"segmented_clothing.jpg\", show_bboxes=True)\n",
        "\n",
        "    mask = results[1]['masks']  # Get the clothing mask\n",
        "\n",
        "    # If 'dress' class is detected, use its mask, otherwise use a blank mask\n",
        "    if mask.shape[2] > 0:\n",
        "        clothing_mask = mask[:, :, 0]  # Get the first mask (assuming it's the dress)\n",
        "    else:\n",
        "        clothing_mask = np.zeros_like(results[0], dtype=np.uint8)\n",
        "\n",
        "    return clothing_mask\n",
        "\n",
        "def create_transparent_clothing_image(image_path, clothing_mask):\n",
        "    \"\"\"Creates a transparent clothing image using the mask.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Create a 4-channel image (BGRA) to store transparency\n",
        "    transparent_image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
        "\n",
        "    # Set alpha channel to 0 where mask is False (background)\n",
        "    transparent_image[clothing_mask == 0, 3] = 0\n",
        "\n",
        "    return transparent_image\n",
        "\n",
        "# Load the clothing image path\n",
        "clothing_image_path = \"/content/drive/My Drive/clothing.jpg\"\n",
        "\n",
        "# Segment the clothing\n",
        "clothing_mask = segment_clothing(clothing_image_path)\n",
        "\n",
        "# Create transparent clothing image\n",
        "transparent_clothing_image = create_transparent_clothing_image(clothing_image_path, clothing_mask)\n",
        "\n",
        "# Display the transparent image (you might need to save it to see the transparency)\n",
        "plt.imshow(cv2.cvtColor(transparent_clothing_image, cv2.COLOR_BGRA2RGBA))\n",
        "plt.title(\"Transparent Clothing Image\")\n",
        "plt.show()\n",
        "\n",
        "# Save the transparent image (optional)\n",
        "cv2.imwrite(\"transparent_clothing.png\", transparent_clothing_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "P0oK6_KZ8iY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show image from person_image_path\n",
        "person_image = cv2.imread(person_image_path)\n",
        "plt.imshow(person_image)"
      ],
      "metadata": {
        "id": "0i8KL3yKw_cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtlFMEO0PGAA"
      },
      "outputs": [],
      "source": [
        "### **3. Clothing Image Warping (Align Clothes to the Person)**\n",
        "\n",
        "# Preprocess person and clothing images\n",
        "# pose_map, segmentation_map = preprocess_person(\"/content/drive/My Drive/person.jpg\")\n",
        "# person_image = preprocess_person(\"/content/drive/My Drive/person.jpg\")\n",
        "# clothing_image = preprocess_clothing(\"/content/drive/My Drive/clothing.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the person image.\n",
        "plt.imshow(person_image)"
      ],
      "metadata": {
        "id": "EH3iCUjxyGNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGYvnGpcPGAB"
      },
      "outputs": [],
      "source": [
        "#### **4. Warping clothing to align with the persons shape**\n",
        "\n",
        "%pip install scikit-image\n",
        "\n",
        "import numpy as np\n",
        "from skimage.transform import PiecewiseAffineTransform, warp\n",
        "import torch\n",
        "\n",
        "# Ensure clothing_image is defined\n",
        "try:\n",
        "    clothing_image\n",
        "except NameError:\n",
        "    raise NameError(\"clothing_image is not defined. Please ensure the cell defining clothing_image is executed.\")\n",
        "\n",
        "# Convert clothing_image to NumPy array if it's a tensor\n",
        "if isinstance(clothing_image, torch.Tensor):\n",
        "    clothing_image = clothing_image.permute(1, 2, 0).numpy()\n",
        "\n",
        "# Define transformation points\n",
        "src_cols = np.linspace(0, clothing_image.shape[1], 10)\n",
        "src_rows = np.linspace(0, clothing_image.shape[0], 10)\n",
        "src_rows, src_cols = np.meshgrid(src_rows, src_cols)\n",
        "src = np.dstack([src_cols.flat, src_rows.flat])[0]\n",
        "\n",
        "# Destination points with slight perturbation\n",
        "dst = src + np.random.normal(0, 5, src.shape)\n",
        "\n",
        "# Apply transformation\n",
        "tform = PiecewiseAffineTransform()\n",
        "tform.estimate(src, dst)\n",
        "warped_clothing = warp(clothing_image, tform)\n",
        "\n",
        "# Convert back to tensor\n",
        "warped_clothing = torch.tensor(warped_clothing).permute(2, 0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMyskuC_PGAB"
      },
      "outputs": [],
      "source": [
        "### **5. Clothing Image Warping (Align Clothes to the Person)**\n",
        "%pip install scikit-image\n",
        "\n",
        "import numpy as np\n",
        "from skimage.transform import PiecewiseAffineTransform, warp\n",
        "\n",
        "# Ensure clothing_image is defined\n",
        "try:\n",
        "  clothing_image\n",
        "except NameError:\n",
        "  raise NameError(\"clothing_image is not defined. Please ensure the cell defining clothing_image is executed.\")\n",
        "\n",
        "# Warping clothing to align with the persons shape\n",
        "tform = PiecewiseAffineTransform()\n",
        "\n",
        "# Assuming pose_map is a set of control points for the transformation\n",
        "# You need to define source and destination control points for the transformation\n",
        "# Here we use dummy control points for illustration\n",
        "src_cols = np.linspace(0, clothing_image.shape[1], 10)\n",
        "src_rows = np.linspace(0, clothing_image.shape[0], 10)\n",
        "src_rows, src_cols = np.meshgrid(src_rows, src_cols)\n",
        "src = np.dstack([src_cols.flat, src_rows.flat])[0]\n",
        "\n",
        "dst = src + np.random.normal(0, 5, src.shape)\n",
        "tform.estimate(src, dst)\n",
        "\n",
        "warped_clothing = warp(clothing_image, tform)\n",
        "warped_clothing = torch.tensor(warped_clothing).permute(2, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import skimage.io as io\n",
        "# import skimage.transform as ski\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def radial_distortion(xy, k1=0.9, k2=0.5, image_shape=None):\n",
        "#     \"\"\"Apply radial distortion to coordinates `xy`.\"\"\"\n",
        "#     if image_shape is None:\n",
        "#         raise ValueError(\"image_shape must be provided for proper normalization.\")\n",
        "\n",
        "#     height, width = image_shape[:2]\n",
        "#     xy_c = np.array([width / 2, height / 2])  # Compute center based on image size\n",
        "\n",
        "#     xy = (xy - xy_c) / np.array([width / 2, height / 2])  # Normalize using image size\n",
        "\n",
        "#     radius = np.linalg.norm(xy, axis=1)  # Compute radial distance\n",
        "#     distortion_model = 1 + k1 * radius**2 + k2 * radius**4  # Correct distortion formula\n",
        "\n",
        "#     xy *= distortion_model[:, np.newaxis]  # Apply distortion\n",
        "#     xy = xy * np.array([width / 2, height / 2]) + xy_c  # Transform back\n",
        "\n",
        "#     return xy\n",
        "\n",
        "# # Pick a few `src` points by hand, and move the corresponding `dst` points to their\n",
        "# # expected positions. Need to align the points with the clothing image so that src and dst points match correclty.\n",
        "# # fmt: off\n",
        "# src = np.array([[22,  22], [100,  10], [177, 22], [190, 100], [177, 177], [100, 188],\n",
        "#                 [22, 177], [ 10, 100], [ 66, 66], [133,  66], [ 66, 133], [133, 133]])\n",
        "# dst = np.array([[ 0,   0], [100,   0], [200,  0], [200, 100], [200, 200], [100, 200],\n",
        "#                 [ 0, 200], [  0, 100], [ 73, 73], [128,  73], [ 73, 128], [128, 128]])\n",
        "# # fmt: on\n",
        "\n",
        "# # Estimate the TPS transformation from these points and then warp the image.\n",
        "# # We switch `src` and `dst` here because `skimage.transform.warp` requires the\n",
        "# # inverse transformation!\n",
        "# tps = ski.ThinPlateSplineTransform() # Use ski instead of ski.transform\n",
        "# tps.estimate(dst, src)\n",
        "# image = io.imread(person_image_path) # Read the image data using io.imread\n",
        "# warped = ski.warp(image, tps) # Use ski instead of ski.transform\n",
        "# image = ski.warp(tps, radial_distortion(xy=src, image_shape=warped), cval=0.0) # This line is not needed here, move to before the plotting code\n",
        "\n",
        "# # Plot the results\n",
        "# fig, axs = plt.subplots(1, 2)\n",
        "# axs[0].imshow(image, cmap='gray')\n",
        "# axs[0].scatter(src[:, 0], src[:, 1], marker='x', color='cyan')\n",
        "# axs[1].imshow(warped, cmap='gray', extent=(0, 200, 200, 0))\n",
        "# axs[1].scatter(dst[:, 0], dst[:, 1], marker='x', color='cyan')\n",
        "\n",
        "# point_labels = [str(i) for i in range(len(src))]\n",
        "# for i, label in enumerate(point_labels):\n",
        "#     axs[0].annotate(\n",
        "#         label,\n",
        "#         (src[:, 0][i], src[:, 1][i]),\n",
        "#         textcoords=\"offset points\",\n",
        "#         xytext=(0, 5),\n",
        "#         ha='center',\n",
        "#         color='red',\n",
        "#     )\n",
        "#     axs[1].annotate(\n",
        "#         label,\n",
        "#         (dst[:, 0][i], dst[:, 1][i]),\n",
        "#         textcoords=\"offset points\",\n",
        "#         xytext=(0, 5),\n",
        "#         ha='center',\n",
        "#         color='red',\n",
        "#     )\n",
        "\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "DxuyMPZX66py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the person image.\n",
        "plt.imshow(person_image)"
      ],
      "metadata": {
        "id": "YzmJKzkbxzK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python\n",
        "!pip install mediapipe --upgrade\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    # Convert to HSV color space\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define a range of white color in HSV\n",
        "    lower_white = np.array([0, 0, 150])  # Adjust these values\n",
        "    upper_white = np.array([180, 50, 255])  # Adjust these values\n",
        "\n",
        "    # Threshold the HSV image to get only white colors\n",
        "    mask = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "    # Apply morphological operations to remove noise and fill gaps\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Find contours of the gown in the mask\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # If contours are found, create a mask with the largest contour\n",
        "    if contours:\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "        gown_mask = np.zeros_like(mask)\n",
        "        cv2.drawContours(gown_mask, [largest_contour], -1, 255, cv2.FILLED)\n",
        "        return gown_mask\n",
        "    else:\n",
        "        return mask  # Return original mask if no contours found\n",
        "\n",
        "\n",
        "# Load images\n",
        "person_image = cv2.imread(person_image_path)\n",
        "clothing_image = cv2.imread(clothing_image_path)\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region using the gown_mask\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# Find contours of the gown in the mask (again, to get accurate bounding box after segmentation)\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Get the largest contour (assuming it's the gown)\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Get bounding rectangle of the gown\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "!pip install mediapipe opencv-python\n",
        "!pip install mediapipe --upgrade\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    # Convert to HSV color space\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define a range of white color in HSV\n",
        "    lower_white = np.array([0, 0, 150])  # Adjust these values\n",
        "    upper_white = np.array([180, 50, 255])  # Adjust these values\n",
        "\n",
        "    # Threshold the HSV image to get only white colors\n",
        "    mask = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "    # Apply morphological operations to remove noise and fill gaps\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Find contours of the gown in the mask\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # If contours are found, create a mask with the largest contour\n",
        "    if contours:\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "        gown_mask = np.zeros_like(mask)\n",
        "        cv2.drawContours(gown_mask, [largest_contour], -1, 255, cv2.FILLED)\n",
        "        return gown_mask\n",
        "    else:\n",
        "        return mask  # Return original mask if no contours found\n",
        "\n",
        "\n",
        "# Load images\n",
        "person_image = cv2.imread(person_image_path)\n",
        "clothing_image = cv2.imread(clothing_image_path)\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region using the gown_mask\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# Find contours of the gown in the mask (again, to get accurate bounding box after segmentation)\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Get the largest contour (assuming it's the gown)\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Get bounding rectangle of the gown\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "        # Define destination points corresponding to the gown coordinates\n",
        "        # Make sure the order matches for correct orientation\n",
        "        dst_pts = np.float32([\n",
        "                [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "                [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "                [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "                [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]],\n",
        "        ])\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    # Convert to HSV color space\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define a range of white color in HSV\n",
        "    lower_white = np.array([0, 0, 150])  # Adjust these values\n",
        "    upper_white = np.array([180, 50, 255])  # Adjust these values\n",
        "\n",
        "    # Threshold the HSV image to get only white colors\n",
        "    mask = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "    # Apply morphological operations to remove noise and fill gaps\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Find contours of the gown in the mask\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # If contours are found, create a mask with the largest contour\n",
        "    if contours:\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "        gown_mask = np.zeros_like(mask)\n",
        "        cv2.drawContours(gown_mask, [largest_contour], -1, 255, cv2.FILLED)\n",
        "        return gown_mask\n",
        "    else:\n",
        "        return mask  # Return original mask if no contours found\n",
        "\n",
        "\n",
        "# Load images\n",
        "person_image = cv2.imread(person_image_path)\n",
        "clothing_image = cv2.imread(clothing_image_path)\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region using the gown_mask\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# Find contours of the gown in the mask (again, to get accurate bounding box after segmentation)\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Get the largest contour (assuming it's the gown)\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Get bounding rectangle of the gown\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "        # Define destination points corresponding to the gown coordinates\n",
        "        # Make sure the order matches for correct orientation\n",
        "        dst_pts = np.float32([\n",
        "                [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "                [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "                [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "                [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]],\n",
        "        ])"
      ],
      "metadata": {
        "id": "zXVmBdYN7QiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pixellib\n",
        "!pip install opencv-python\n",
        "\n",
        "import pixellib\n",
        "from pixellib.instance import instance_segmentation\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def segment_person(image_path):\n",
        "  \"\"\"Segments the person in the image using PixelLib.\"\"\"\n",
        "  segment_image = instance_segmentation()\n",
        "  segment_image.load_model(\"mask_rcnn_coco.h5\")  # Download the model: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "  results = segment_image.segmentImage(image_path, output_image_name=\"segmented_person.jpg\", show_bboxes=True)\n",
        "  mask = results[1]['masks'][:, :, 0]  # Get the person mask\n",
        "  return mask\n",
        "\n",
        "def remove_clothing_and_inpaint(image, mask):\n",
        "  \"\"\"Removes clothing using the mask and inpaints the removed area.\"\"\"\n",
        "  # Create an inverted mask to select the clothing area\n",
        "  clothing_mask = ~mask.astype(bool)\n",
        "\n",
        "  # Set the clothing area to black (or any desired background color)\n",
        "  image[clothing_mask] = [0, 0, 0]  # Black background\n",
        "\n",
        "  # Inpaint the removed area\n",
        "  inpainted_image = cv2.inpaint(image, clothing_mask.astype(np.uint8), 3, cv2.INPAINT_TELEA)\n",
        "  return inpainted_image\n",
        "\n",
        "# Load the images\n",
        "person_image = cv2.imread(person_image_path)\n",
        "clothing_image = cv2.imread(clothing_image_path)\n",
        "\n",
        "# 1. Person Segmentation\n",
        "person_mask = segment_person(person_image_path)\n",
        "\n",
        "# 2. Clothing Removal and Inpainting\n",
        "person_image_no_clothing = remove_clothing_and_inpaint(person_image.copy(), person_mask)\n",
        "\n",
        "# 3. Gown Segmentation (using approximate_gown_segmentation function from previous code)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region using the gown_mask\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# ... (rest of the code for perspective transformation and blending remains the same,\n",
        "# but use person_image_no_clothing instead of the original person_image)"
      ],
      "metadata": {
        "id": "3v0ts3JU8QSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mediapipe opencv-python\n",
        "# !pip install mediapipe --upgrade\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define a range of white color in HSV\n",
        "    lower_white = np.array([0, 0, 150])  # Adjust these values\n",
        "    upper_white = np.array([180, 50, 255])  # Adjust these values\n",
        "\n",
        "    # Threshold the HSV image to get only white colors\n",
        "    mask = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "    # Apply morphological operations to remove noise and fill gaps\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# Load images\n",
        "# person_image = io.imread(person_image_path)\n",
        "# clothing_image = io.imread(clothing_image_path)\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region using the gown_mask\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# Find contours of the gown in the mask\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Get the largest contour (assuming it's the gown)\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Get bounding rectangle of the gown\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "        # Print person image.\n",
        "        person_image = cv2.imread(person_image_path)\n",
        "        plt.imshow(person_image)\n",
        "\n",
        "        # Define destination points corresponding to the gown coordinates\n",
        "        # Make sure the order matches for correct orientation\n",
        "        dst_pts = np.float32([\n",
        "                [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "                [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "                [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "                [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]]\n",
        "            ])\n",
        "\n",
        "        # Calculate the perspective transformation matrix\n",
        "        M = cv2.getPerspectiveTransform(gown_coords, dst_pts)\n",
        "\n",
        "        # Warp the gown image\n",
        "        warped_gown = cv2.warpPerspective(clothing_image, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "        # 4. Blending with background correction\n",
        "        # Create a mask for the warped gown (non-black pixels)\n",
        "        warped_gown_mask = cv2.cvtColor(warped_gown, cv2.COLOR_BGR2GRAY)\n",
        "        warped_gown_mask = warped_gown_mask > 0  # Convert to binary mask\n",
        "\n",
        "        # Apply the mask to the person image to replace the area where the gown will be placed\n",
        "        person_image_with_hole = person_image.copy()\n",
        "        person_image_with_hole[warped_gown_mask] = warped_gown[warped_gown_mask]\n",
        "\n",
        "        # 5. Display Results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(clothing_image)\n",
        "        plt.title(\"Original Clothing Image\")\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(person_image_with_hole) # Show the blended image\n",
        "        plt.title(\"Warped and Blended Image\")\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Pose landmarks not detected in the person image.\")\n",
        "else:\n",
        "    print(\"No contours detected in the clothing image. Check segmentation.\")"
      ],
      "metadata": {
        "id": "AI4jQ9rv6i1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mediapipe opencv-python\n",
        "# !pip install mediapipe --upgrade\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define a range of white color in HSV\n",
        "    lower_white = np.array([0, 0, 150])  # Adjust these values\n",
        "    upper_white = np.array([180, 50, 255])  # Adjust these values\n",
        "\n",
        "    # Threshold the HSV image to get only white colors\n",
        "    mask = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "    # Apply morphological operations to remove noise and fill gaps\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "    # lower_color = np.array([0, 0, 150])  # Adjust these values\n",
        "    # upper_color = np.array([180, 50, 255])  # Adjust these values\n",
        "    # mask = cv2.inRange(hsv, lower_color, upper_color)\n",
        "    # kernel = np.ones((5, 5), np.uint8)\n",
        "    # mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    # mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    # return mask\n",
        "\n",
        "# Load images\n",
        "# person_image = io.imread(person_image_path)\n",
        "# clothing_image = io.imread(clothing_image_path)\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region using the gown_mask\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# Find contours of the gown in the mask\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Get the largest contour (assuming it's the gown)\n",
        "if contours:\n",
        "  largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "  # Get bounding rectangle of the gown\n",
        "  x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "  gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "  # x, y, w, h = cv2.boundingRect(gown_mask)\n",
        "  # gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "  # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "  if results.pose_landmarks:\n",
        "    left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "    right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "    left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "    right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "# Print person image.\n",
        "person_image = cv2.imread(person_image_path)\n",
        "plt.imshow(person_image)\n",
        "\n",
        "  # Define destination points corresponding to the gown coordinates\n",
        "  # Make sure the order matches for correct orientation\n",
        "dst_pts = np.float32([\n",
        "        [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "        [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "        [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "        [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]]\n",
        "    ])\n",
        "\n",
        "    # Calculate the perspective transformation matrix\n",
        "M = cv2.getPerspectiveTransform(gown_coords, dst_pts)\n",
        "\n",
        "# Warp the gown image\n",
        "warped_gown = cv2.warpPerspective(clothing_image, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "# 4. Blending with background correction\n",
        "# Create a mask for the warped gown (non-black pixels)\n",
        "warped_gown_mask = cv2.cvtColor(warped_gown, cv2.COLOR_BGR2GRAY)\n",
        "warped_gown_mask = warped_gown_mask > 0  # Convert to binary mask\n",
        "\n",
        "# Apply the mask to the person image to replace the area where the gown will be placed\n",
        "person_image_with_hole = person_image.copy()\n",
        "person_image_with_hole[warped_gown_mask] = warped_gown[warped_gown_mask]\n",
        "\n",
        "    # # 4. Blending\n",
        "    # alpha = 0.7\n",
        "    # blended_image = cv2.addWeighted(person_image, 1 - alpha, warped_gown, alpha, 0)\n",
        "\n",
        "    # 5. Display Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(clothing_image)\n",
        "plt.title(\"Original Clothing Image\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(person_image_with_hole) # Show the blended image\n",
        "plt.title(\"Warped and Blended Image\")\n",
        "# !pip install mediapipe opencv-python\n",
        "# !pip install mediapipe --upgrade\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define a range of white color in HSV\n",
        "    lower_white = np.array([0, 0, 150])  # Adjust these values\n",
        "    upper_white = np.array([180, 50, 255])  # Adjust these values\n",
        "\n",
        "    # Threshold the HSV image to get only white colors\n",
        "    mask = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "    # Apply morphological operations to remove noise and fill gaps\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# Load images\n",
        "# person_image = io.imread(person_image_path)\n",
        "# clothing_image = io.imread(clothing_image_path)\n",
        "\n",
        "# 1. Pose Estimation (using MediaPipe)\n",
        "mp_pose = mp.solutions.pose.Pose()\n",
        "# Placeholder for person_image - replace with actual image loading\n",
        "person_image = np.zeros((100,100,3), dtype=np.uint8) # Dummy Image\n",
        "results = mp_pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# 2. Gown Coordinate Detection (using color thresholding)\n",
        "# Placeholder for clothing_image - replace with actual image loading\n",
        "clothing_image = np.zeros((100,100,3), dtype=np.uint8) # Dummy Image\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region using the gown_mask\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# Find contours of the gown in the mask\n",
        "contours, _ = cv2.findContours(gown_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Get the largest contour (assuming it's the gown)\n",
        "if contours:\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Get bounding rectangle of the gown\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    gown_coords = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "\n",
        "\n",
        "    # 3. Perspective Transformation (if pose landmarks are detected)\n",
        "    if results.pose_landmarks:\n",
        "        left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]\n",
        "        right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "        left_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_HIP]\n",
        "        right_hip = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "    # Placeholder for person_image_path\n",
        "    person_image_path = \"\" # Replace with actual path\n",
        "    # Print person image.\n",
        "    person_image = cv2.imread(person_image_path)\n",
        "    if person_image is None:\n",
        "        print(\"Error: Could not read person image.\")\n",
        "\n",
        "    # Define destination points corresponding to the gown coordinates\n",
        "    # Make sure the order matches for correct orientation\n",
        "    dst_pts = np.float32([\n",
        "            [left_shoulder.x * person_image.shape[1], left_shoulder.y * person_image.shape[0]],\n",
        "            [right_shoulder.x * person_image.shape[1], right_shoulder.y * person_image.shape[0]],\n",
        "            [right_hip.x * person_image.shape[1], right_hip.y * person_image.shape[0]],\n",
        "            [left_hip.x * person_image.shape[1], left_hip.y * person_image.shape[0]]\n",
        "        ])\n",
        "\n",
        "    # Calculate the perspective transformation matrix\n",
        "    M = cv2.getPerspectiveTransform(gown_coords, dst_pts)\n",
        "\n",
        "    # Warp the gown image\n",
        "    warped_gown = cv2.warpPerspective(clothing_image, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "    # 4. Blending with background correction\n",
        "    # Create a mask for the warped gown (non-black pixels)\n",
        "    warped_gown_mask = cv2.cvtColor(warped_gown, cv2.COLOR_BGR2GRAY)\n",
        "    warped_gown_mask = warped_gown_mask > 0  # Convert to binary mask\n",
        "\n",
        "    # Apply the mask to the person image to replace the area where the gown will be placed\n",
        "    person_image_with_hole = person_image.copy()\n",
        "    person_image_with_hole[warped_gown_mask] = warped_gown[warped_gown_mask]\n",
        "\n",
        "\n",
        "    # 5. Display Results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(clothing_image)\n",
        "    plt.title(\"Original Clothing Image\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(person_image_with_hole) # Show the blended image\n",
        "    plt.title(\"Warped and Blended Image\")\n",
        "    plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Pose landmarks not detected in the person image.\")\n",
        "else:\n",
        "    print(\"No contours detected in the clothing image. Check segmentation.\")\n",
        "  else:\n",
        "print(\"No contours detected in the clothing image. Check segmentation.\")"
      ],
      "metadata": {
        "id": "VDU2A5a2M1GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running:\n",
        "\n",
        "# Replace placeholders: Make sure to replace person_image_path and clothing_image_path with the actual paths to your person and clothing images.\n",
        "# Adjust destination points: The dst_pts values are just examples. You'll likely need to adjust them carefully to align the gown correctly with the person's body in your specific images. You can use an image editor to get the pixel coordinates for the destination points.\n",
        "# Refine segmentation (if needed): If the color thresholding in approximate_gown_segmentation doesn't give you a good segmentation, consider using a more robust method (pre-trained segmentation models or manual annotation) and replace that function accordingly.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def approximate_gown_segmentation(image):\n",
        "    \"\"\"Approximates gown segmentation using color thresholding.\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    lower_color = np.array([0, 0, 150])\n",
        "    upper_color = np.array([180, 50, 255])\n",
        "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# Load images\n",
        "person_image = io.imread(person_image_path)  # Replace with your person image path\n",
        "clothing_image = io.imread(clothing_image_path)  # Replace with your clothing image path\n",
        "\n",
        "# Approximate gown segmentation\n",
        "gown_mask = approximate_gown_segmentation(clothing_image)\n",
        "\n",
        "# Extract Gown Region\n",
        "gown_image = cv2.bitwise_and(clothing_image, clothing_image, mask=gown_mask)\n",
        "\n",
        "# Find Gown Bounding Box\n",
        "x, y, w, h = cv2.boundingRect(gown_mask)\n",
        "\n",
        "# Define Source and Destination Points\n",
        "src_pts = np.float32([[x, y], [x + w, y], [x + w, y + h], [x, y + h]])\n",
        "dst_pts = np.float32([[100, 150], [200, 150], [200, 350], [100, 350]])  # Adjust as needed\n",
        "\n",
        "# Calculate Perspective Transformation Matrix\n",
        "M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
        "\n",
        "# Warp the Gown Image\n",
        "warped_gown = cv2.warpPerspective(gown_image, M, (person_image.shape[1], person_image.shape[0]))\n",
        "\n",
        "# Blend (Optional)\n",
        "alpha = 0.7\n",
        "blended_image = cv2.addWeighted(person_image, 1 - alpha, warped_gown, alpha, 0)\n",
        "\n",
        "# Display Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(clothing_image)\n",
        "plt.title(\"Original Clothing Image\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(blended_image)\n",
        "plt.title(\"Warped and Blended Image\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hTt5l1nLL-5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import skimage.transform as ski\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def radial_distortion(xy, k1=0.9, k2=0.5, image_shape=None):\n",
        "    \"\"\"Apply radial distortion to coordinates `xy`.\"\"\"\n",
        "    if image_shape is None:\n",
        "        raise ValueError(\"image_shape must be provided for proper normalization.\")\n",
        "\n",
        "    height, width = image_shape[:2]\n",
        "    xy_c = np.array([width / 2, height / 2])  # Compute center based on image size\n",
        "\n",
        "    xy = (xy - xy_c) / np.array([width / 2, height / 2])  # Normalize using image size\n",
        "\n",
        "    radius = np.linalg.norm(xy, axis=1)  # Compute radial distance\n",
        "    distortion_model = 1 + k1 * radius**2 + k2 * radius**4  # Correct distortion formula\n",
        "\n",
        "    xy *= distortion_model[:, np.newaxis]  # Apply distortion\n",
        "    xy = xy * np.array([width / 2, height / 2]) + xy_c  # Transform back\n",
        "\n",
        "    return xy\n",
        "\n",
        "# def radial_distortion(xy, k1=0.9, k2=0.5):\n",
        "#     \"\"\"Apply radial distortion to coordinates `xy`.\"\"\"\n",
        "#     xy_c = xy.mean(axis=0)  # Correctly compute the center\n",
        "#     xy = (xy - xy_c) / np.linalg.norm(xy_c)  # Normalize\n",
        "\n",
        "#     radius = np.linalg.norm(xy, axis=1)  # Compute radial distance\n",
        "#     distortion_model = 1 + k1 * radius**2 + k2 * radius**4  # Correct distortion formula\n",
        "\n",
        "#     xy *= distortion_model[:, np.newaxis]  # Apply distortion\n",
        "#     xy = xy * np.linalg.norm(xy_c) + xy_c  # Transform back\n",
        "\n",
        "#     return xy\n",
        "\n",
        "image = io.imread(person_image_path) # Read the image data using io.imread\n",
        "image = ski.warp(image, radial_distortion, cval=0.0) # This line is not needed here, move to before the plotting code\n",
        "# Pick a few `src` points by hand, and move the corresponding `dst` points to their\n",
        "# expected positions. Need to align the points with the clothing image so that src and dst points match correclty.\n",
        "# fmt: off\n",
        "src = np.array([[22,  22], [100,  10], [177, 22], [190, 100], [177, 177], [100, 188],\n",
        "                [22, 177], [ 10, 100], [ 66, 66], [133,  66], [ 66, 133], [133, 133]])\n",
        "dst = np.array([[ 0,   0], [100,   0], [200,  0], [200, 100], [200, 200], [100, 200],\n",
        "                [ 0, 200], [  0, 100], [ 73, 73], [128,  73], [ 73, 128], [128, 128]])\n",
        "# fmt: on\n",
        "\n",
        "# Estimate the TPS transformation from these points and then warp the image.\n",
        "# We switch `src` and `dst` here because `skimage.transform.warp` requires the\n",
        "# inverse transformation!\n",
        "tps = ski.ThinPlateSplineTransform() # Use ski instead of ski.transform\n",
        "tps.estimate(dst, src)\n",
        "warped = ski.warp(image, tps) # Use ski instead of ski.transform\n",
        "\n",
        "# Plot the results\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(image, cmap='gray')\n",
        "axs[0].scatter(src[:, 0], src[:, 1], marker='x', color='cyan')\n",
        "axs[1].imshow(warped, cmap='gray', extent=(0, 200, 200, 0))\n",
        "axs[1].scatter(dst[:, 0], dst[:, 1], marker='x', color='cyan')\n",
        "\n",
        "point_labels = [str(i) for i in range(len(src))]\n",
        "for i, label in enumerate(point_labels):\n",
        "    axs[0].annotate(\n",
        "        label,\n",
        "        (src[:, 0][i], src[:, 1][i]),\n",
        "        textcoords=\"offset points\",\n",
        "        xytext=(0, 5),\n",
        "        ha='center',\n",
        "        color='red',\n",
        "    )\n",
        "    axs[1].annotate(\n",
        "        label,\n",
        "        (dst[:, 0][i], dst[:, 1][i]),\n",
        "        textcoords=\"offset points\",\n",
        "        xytext=(0, 5),\n",
        "        ha='center',\n",
        "        color='red',\n",
        "    )\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0ca33Hat4BaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 6. Generate Try-On Image with Thin-Plate-Spline Normalization\n",
        "\n",
        "# # Install the correct package\n",
        "# !pip install py-thin-plate-spline\n",
        "\n",
        "# # Import necessary libraries\n",
        "# # 6. Generate Try-On Image with Thin-Plate-Spline Normalization\n",
        "\n",
        "# # Install the correct package\n",
        "# !pip install py-thin-plate-spline\n",
        "\n",
        "# # Import necessary libraries\n",
        "# import numpy as np\n",
        "# from scipy.interpolate import Rbf\n",
        "# from thin_plate_spline import ThinPlateSpline\n",
        "\n",
        "# # Define source and target points for TPS transformation\n",
        "# source_points = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
        "# target_points = np.array([[0, 0], [2, 0], [0, 2], [2, 2]])\n",
        "\n",
        "# # Initialize Thin Plate Spline transformation\n",
        "# # Pass source and target points as a dictionary\n",
        "# tps = ThinPlateSpline(alpha=0.1) # Added alpha paramter for regularization. Adjust as needed.\n",
        "\n",
        "# # Fit the TPS transformation to the source and target points\n",
        "# # Pass source and target points to the fit method\n",
        "# tps.fit(source_points, target_points)\n",
        "\n",
        "# # Transform new points\n",
        "# new_points = np.array([[0.5, 0.5], [0.25, 0.75]])\n",
        "# transformed_points = tps.transform(new_points)\n",
        "\n",
        "# # Alternative method using SciPy Rbf\n",
        "# x = np.random.rand(10)\n",
        "# y = np.random.rand(10)\n",
        "# z = np.random.rand(10)\n",
        "\n",
        "# # Create TPS function with SciPy\n",
        "# tps_rbf = Rbf(x, y, z, function='thin_plate')\n",
        "\n",
        "# # Evaluate at new points\n",
        "# x_new, y_new = np.random.rand(5), np.random.rand(5)\n",
        "# z_new = tps_rbf(x_new, y_new)\n",
        "\n",
        "# # Print results\n",
        "# print(\"Thin Plate Spline (thin-plate-spline package) Output:\")\n",
        "# print(transformed_points)\n",
        "\n",
        "# print(\"\\nThin Plate Spline (SciPy Rbf) Output:\")\n",
        "# print(z_new)\n",
        "# source_points = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
        "# target_points = np.array([[0, 0], [2, 0], [0, 2], [2, 2]])\n",
        "\n",
        "# # Initialize Thin Plate Spline transformation\n",
        "# # Pass source and target points as a dictionary\n",
        "# tps = ThinPlateSpline(alpha=0.1) # Added alpha paramter for regularization. Adjust as needed.\n",
        "\n",
        "# # Fit the TPS transformation to the source and target points\n",
        "# # Pass source and target points to the fit method\n",
        "# tps.fit(source_points, target_points)\n",
        "\n",
        "# # Transform new points\n",
        "# new_points = np.array([[0.5, 0.5], [0.25, 0.75]])\n",
        "# transformed_points = tps.transform(new_points)\n",
        "\n",
        "# # Alternative method using SciPy Rbf\n",
        "# x = np.random.rand(10)\n",
        "# y = np.random.rand(10)\n",
        "# z = np.random.rand(10)\n",
        "\n",
        "# # Create TPS function with SciPy\n",
        "# tps_rbf = Rbf(x, y, z, function='thin_plate')\n",
        "\n",
        "# # Evaluate at new points\n",
        "# x_new, y_new = np.random.rand(5), np.random.rand(5)\n",
        "# z_new = tps_rbf(x_new, y_new)\n",
        "\n",
        "# # Print results\n",
        "# print(\"Thin Plate Spline (thin-plate-spline package) Output:\")\n",
        "# print(transformed_points)\n",
        "\n",
        "# print(\"\\nThin Plate Spline (SciPy Rbf) Output:\")\n",
        "# print(z_new)\n"
      ],
      "metadata": {
        "id": "DWrjdsJ84D3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. Generate Try-On Image with Thin-Plate-Spline Normalization\n",
        "\n",
        "# import ThinPlateSpline as ThinPlateSplineTransform\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.interpolate import Rbf\n",
        "import torch\n",
        "# from tps import ThinPlateSpline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load images\n",
        "# person_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/person.jpg\"  # Change to actual person image path\n",
        "# cloth_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/clothing.jpg\"  # Change to actual clothing image path\n",
        "\n",
        "# person_img = cv2.imread(person_img_path)\n",
        "# cloth_img = cv2.imread(cloth_img_path)\n",
        "\n",
        "# if person_image is None or cloth_img is None:\n",
        "#     raise FileNotFoundError(\"One of the input images was not found.\")\n",
        "\n",
        "# # Example Key Points (Adjust based on your dataset)\n",
        "person_points = np.array([\n",
        "    [100, 50],  # Shoulder Left\n",
        "    [200, 50],  # Shoulder Right\n",
        "    [120, 150], # Waist Left\n",
        "    [180, 150], # Waist Right\n",
        "])\n",
        "\n",
        "cloth_points = np.array([\n",
        "    [90, 40],   # Neck Left\n",
        "    [210, 40],  # Neck Right\n",
        "    [110, 140], # Bottom Left\n",
        "    [190, 140], # Bottom Right\n",
        "])\n",
        "\n",
        "# # Function for TPS Warping using SciPy\n",
        "def thin_plate_spline_warp(cloth_img, src_points, dst_points):\n",
        "    height, width = cloth_img.shape[:2]\n",
        "    grid_x, grid_y = np.meshgrid(np.arange(width), np.arange(height))\n",
        "\n",
        "    src_x, src_y = src_points[:, 0], src_points[:, 1]\n",
        "    dst_x, dst_y = dst_points[:, 0], dst_points[:, 1]\n",
        "\n",
        "    tps_x = Rbf(src_x, src_y, dst_x, function='thin_plate')\n",
        "    tps_y = Rbf(src_x, src_y, dst_y, function='thin_plate')\n",
        "\n",
        "    map_x = tps_x(grid_x, grid_y).astype(np.float32)\n",
        "    map_y = tps_y(grid_x, grid_y).astype(np.float32)\n",
        "\n",
        "    warped_cloth = cv2.remap(cloth_img, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n",
        "    return warped_cloth\n",
        "\n",
        "# Apply TPS warping\n",
        "warped_clothing = thin_plate_spline_warp(clothing_image, cloth_points, person_points)\n",
        "\n",
        "# Save & Show Warped Clothing\n",
        "cv2.imwrite(\"warped_clothing.jpg\", warped_clothing)\n",
        "plt.imshow(cv2.cvtColor(warped_clothing, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Warped Clothing\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Blending the Warped Clothing with Person Image\n",
        "# Load person image again cv2.imread\n",
        "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
        "person_image_for_blending = cv2.imread(person_image_path)\n",
        "\n",
        "warped_clothing = cv2.resize(warped_clothing, (person_image_for_blending.shape[1], person_image_for_blending.shape[0]))\n",
        "alpha = 0.7  # Transparency Level\n",
        "\n",
        "# Convert warped_clothing to the same data type as person_image_for_blending\n",
        "warped_clothing = warped_clothing.astype(person_image_for_blending.dtype)\n",
        "\n",
        "blended = cv2.addWeighted(person_image_for_blending, 1 - alpha, warped_clothing, alpha, 0)\n",
        "\n",
        "# Save & Show Final Output\n",
        "cv2.imwrite(\"final_tryon.jpg\", blended)\n",
        "plt.imshow(cv2.cvtColor(blended, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Final Try-On\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Ensure warped_clothing and segmentation_map are defined\n",
        "try:\n",
        "    warped_clothing\n",
        "    segmentation_map = np.zeros_like(warped_clothing) #dummy assignment to avoid error\n",
        "except NameError:\n",
        "    raise NameError(\"warped_clothing or segmentation_map is not defined. Please ensure the cells defining them are executed.\")\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from scipy.interpolate import Rbf\n",
        "# import torch\n",
        "# from tps import ThinPlateSpline\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# Load images\n",
        "# person_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/person.jpg\"  # Change to actual person image path\n",
        "# cloth_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/clothing.jpg\"  # Change to actual clothing image path\n",
        "\n",
        "# person_img = cv2.imread(person_img_path)\n",
        "# cloth_img = cv2.imread(cloth_img_path)\n",
        "\n",
        "# if person_img is None or cloth_img is None:\n",
        "#     raise FileNotFoundError(\"One of the input images was not found.\")\n",
        "\n",
        "# # Example Key Points (Adjust based on your dataset)\n",
        "# person_points = np.array([\n",
        "#     [100, 50],  # Shoulder Left\n",
        "#     [200, 50],  # Shoulder Right\n",
        "#     [120, 150], # Waist Left\n",
        "#     [180, 150], # Waist Right\n",
        "# ])\n",
        "\n",
        "# cloth_points = np.array([\n",
        "#     [90, 40],   # Neck Left\n",
        "#     [210, 40],  # Neck Right\n",
        "#     [110, 140], # Bottom Left\n",
        "#     [190, 140], # Bottom Right\n",
        "# ])\n",
        "\n",
        "# # Function for TPS Warping using SciPy\n",
        "# def thin_plate_spline_warp(cloth_img, src_points, dst_points):\n",
        "#     height, width = cloth_img.shape[:2]\n",
        "#     grid_x, grid_y = np.meshgrid(np.arange(width), np.arange(height))\n",
        "\n",
        "#     src_x, src_y = src_points[:, 0], src_points[:, 1]\n",
        "#     dst_x, dst_y = dst_points[:, 0], dst_points[:, 1]\n",
        "\n",
        "#     tps_x = Rbf(src_x, src_y, dst_x, function='thin_plate')\n",
        "#     tps_y = Rbf(src_x, src_y, dst_y, function='thin_plate')\n",
        "\n",
        "#     map_x = tps_x(grid_x, grid_y).astype(np.float32)\n",
        "#     map_y = tps_y(grid_x, grid_y).astype(np.float32)\n",
        "\n",
        "#     warped_cloth = cv2.remap(cloth_img, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n",
        "#     return warped_cloth\n",
        "\n",
        "# # Apply TPS warping\n",
        "# warped_clothing = thin_plate_spline_warp(cloth_img, cloth_points, person_points)\n",
        "\n",
        "# # Save & Show Warped Clothing\n",
        "# cv2.imwrite(\"warped_clothing.jpg\", warped_clothing)\n",
        "# plt.imshow(cv2.cvtColor(warped_clothing, cv2.COLOR_BGR2RGB))\n",
        "# plt.title(\"Warped Clothing\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()\n",
        "\n",
        "# # Blending the Warped Clothing with Person Image\n",
        "# warped_clothing = cv2.resize(warped_clothing, (person_img.shape[1], person_img.shape[0]))\n",
        "# alpha = 0.7  # Transparency Level\n",
        "# blended = cv2.addWeighted(person_img, 1 - alpha, warped_clothing, alpha, 0)\n",
        "\n",
        "# # Save & Show Final Output\n",
        "# cv2.imwrite(\"final_tryon.jpg\", blended)\n",
        "# plt.imshow(cv2.cvtColor(blended, cv2.COLOR_BGR2RGB))\n",
        "# plt.title(\"Final Try-On\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()\n",
        "\n",
        "# # Ensure warped_clothing and segmentation_map are defined\n",
        "# try:\n",
        "#     warped_clothing\n",
        "#     segmentation_map\n",
        "# except NameError:\n",
        "#     raise NameError(\"warped_clothing or segmentation_map is not defined. Please ensure the cells defining them are executed.\")\n"
      ],
      "metadata": {
        "id": "-76faGJc91rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON1m4fSfPGAB"
      },
      "outputs": [],
      "source": [
        "### **4. Generate Try-On Image with ALIAS Normalization**\n",
        "\n",
        "!pip install alias_generator\n",
        "\n",
        "# Import viton-hd\n",
        "!pip install viton-hd\n",
        "\n",
        "# from alias_generator import ALIASGenerator\n",
        "from viton_hd.hd_model import ALIASGenerator\n",
        "\n",
        "# Ensure warped_clothing and segmentation_map are defined\n",
        "try:\n",
        "    warped_clothing\n",
        "    segmentation_map\n",
        "except NameError:\n",
        "    raise NameError(\"warped_clothing or segmentation_map is not defined. Please ensure the cells defining them are executed.\")\n",
        "\n",
        "# Using the ALIAS Generator to combine warped clothing and person image\n",
        "alias_generator = ALIASGenerator()\n",
        "output_image = alias_generator.generate(warped_clothing, segmentation_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml-1PqqZPGAB"
      },
      "outputs": [],
      "source": [
        "### **5. Train the Model (if needed)**\n",
        "\n",
        "import torch\n",
        "\n",
        "# Define dummy dataloader, adversarial_loss, perceptual_loss, and ground_truth for the example\n",
        "dataloader = [(\"person_data\", clothing_image)]  # Dummy dataloader\n",
        "adversarial_loss = lambda output, target: torch.tensor(0.0)  # Dummy loss function\n",
        "perceptual_loss = lambda output, target: torch.tensor(0.0)  # Dummy loss function\n",
        "ground_truth = torch.zeros_like(clothing_image)  # Dummy ground truth\n",
        "\n",
        "# Define the tryon_model architecture\n",
        "class TryOnModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TryOnModel, self).__init__()\n",
        "        # Example model architecture with convolutional layers\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=6, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc = torch.nn.Linear(in_features=512*16*12, out_features=3*64*48)\n",
        "\n",
        "    def forward(self, clothing_data, person_data):\n",
        "        # Example forward pass\n",
        "        x = torch.cat((clothing_data, person_data), dim=1)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        output = torch.sigmoid(self.fc(x))\n",
        "        output = output.view(-1, 3, 64, 48)\n",
        "        return output\n",
        "\n",
        "# Initialize the tryon_model\n",
        "tryon_model = TryOnModel()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(tryon_model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        person_data, clothing_data = batch\n",
        "        output = tryon_model(clothing_data, person_data)\n",
        "        loss = adversarial_loss(output, ground_truth) + perceptual_loss(output, ground_truth)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us48vXLXPGAC"
      },
      "outputs": [],
      "source": [
        "### **6. Run Inference**\n",
        "\n",
        "# Load trained model\n",
        "try:\n",
        "    tryon_model.load_state_dict(torch.load(\"viton_hd_weights.pth\"))\n",
        "    tryon_model.eval()\n",
        "except FileNotFoundError:\n",
        "    print(\"Weights file not found. Using dummy weights for testing.\")\n",
        "    dummy_state_dict = tryon_model.state_dict()\n",
        "    tryon_model.load_state_dict(dummy_state_dict)\n",
        "\n",
        "# Ensure warped_clothing is defined\n",
        "try:\n",
        "    warped_clothing\n",
        "except NameError:\n",
        "    print(\"warped_clothing is not defined. Please ensure the cell generating warped_clothing is executed.\")\n",
        "    # Dummy warped_clothing for testing\n",
        "    warped_clothing = torch.zeros_like(clothing_image)\n",
        "\n",
        "# Generate try-on result\n",
        "with torch.no_grad():\n",
        "    output_image = tryon_model(warped_clothing, segmentation_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q34M6LquPGAC"
      },
      "outputs": [],
      "source": [
        "### **7. Display Results**\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the output_image is defined\n",
        "try:\n",
        "\tplt.imshow(output_image.permute(1, 2, 0))\n",
        "\tplt.axis(\"off\")\n",
        "\tplt.show()\n",
        "except NameError:\n",
        "\tprint(\"output_image is not defined. Please ensure the cell generating output_image is executed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}